{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DATA\\a.asri\\doc-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DATA\\a.asri\\doc-classifier\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_dir = %pwd\n",
    "project_dir = os.path.dirname(current_dir)\n",
    "%cd $project_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00 - Préambule ###\n",
    "L'objectif de cette étape est la modélisation des données de texte sans structure (cf. notebook 08-2).\n",
    "Les modèles ayant obtenu les scores les plus elevés ont été retenu pour la suite : \n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest Classfier\n",
    "- Nearest Centroid\n",
    "- Extra Trees Classifier\n",
    "- XGBClassifier\n",
    "- LGBMClassifier\n",
    "\n",
    "Le paramétrage des modèles choisis via les grilles de recherches (GridSearch) permettra ,\n",
    "d'avoir des niveau de précision plus elevés, et permettre d'hiérarchiser encore plus chacun des modèles entraînés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des bibliothèques nécessaires\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import load\n",
    "\n",
    "#Chargement des données et préparation des données features et target :\n",
    "\n",
    "df = pd.read_csv(\"data\\processed\\words_structure.csv\") \n",
    "seed = 42\n",
    "df['words'] = df['words'].fillna('')\n",
    "\n",
    "target = df['category']\n",
    "features = df.drop('category', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répértoire des outputs\n",
    "save_dir = \"models\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 - Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Chargement du LabelEncoder préalablement sauvegardé\n",
    "le = load(\"models/target_LabelEncoder.joblib\")\n",
    "\n",
    "\"\"\"Selon l'encoder enregistré : \n",
    "    email :                     0\n",
    "    handwritten :               1\n",
    "    invoice :                   2\n",
    "    national_identity_card :    3\n",
    "    passeport :                 4\n",
    "    scientific_publication :    5\n",
    "\"\"\"\n",
    "\n",
    "# Chargement du TfidfVectorizer préalablement sauvegardé\n",
    "tfidf_vectorizer = load('models/TfidfVectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des étiquettes de classe\n",
    "y_train_encoded = le.transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Extraction des données textuelles et conversion en listes\n",
    "X_train_corpus = X_train['words'].tolist()\n",
    "X_test_corpus = X_test['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des données textuelles en vecteurs TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train_corpus)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "invoice                   1770\n",
      "passeport                 1397\n",
      "handwritten               1250\n",
      "email                     1250\n",
      "scientific_publication    1250\n",
      "national_identity_card     192\n",
      "Name: count, dtype: int64\n",
      " \n",
      "Nous constatons que le jeu de données est plus au moins équilibré sauf pour la catégorie 'national_identity_card', \n",
      " nous pourrions utiliser L'Over Sampling pour équilibrer encore plus notre je de données\n"
     ]
    }
   ],
   "source": [
    "occurrences = df['category'].value_counts()\n",
    "print(occurrences)\n",
    "\n",
    "print(\" \\nNous constatons que le jeu de données est plus au moins équilibré sauf pour la catégorie 'national_identity_card', \\n \\\n",
    "nous pourrions utiliser L'Over Sampling pour équilibrer encore plus notre je de données\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Enregistrer le modèle dans le répertoire \"models\"\n",
    "save_dir = \"models\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 - Modélisation des données\n",
    "\n",
    "Les modèles retenus suite aux résultats de la bibliothèque \"lazypredict\" sont :\n",
    "\n",
    "| Modèle                   | Accuracy | Balanced Accuracy | ROC AUC | F1 Score | Time Taken (s) |\n",
    "|--------------------------|----------|-------------------|---------|----------|----------------|\n",
    "| LogisticRegression       | 0.86     | 0.82              | None    | 0.86     | 95.99          |\n",
    "| NearestCentroid          | 0.85     | 0.81              | None    | 0.85     | 95.60          |\n",
    "| ExtraTreesClassifier     | 0.86     | 0.81              | None    | 0.86     | 256.07         |\n",
    "| RandomForestClassifier   | 0.84     | 0.79              | None    | 0.84     | 190.07         |\n",
    "| XGBClassifier            | 0.82     | 0.77              | None    | 0.82     | 322.53         |\n",
    "| LGBMClassifier           | 0.82     | 0.76              | None    | 0.82     | 98.41          |\n",
    "\n",
    "Ces modèles ont été sélectionnés en fonction de leurs performances en termes de précision (Accuracy) et de scores F1, ainsi que de leur temps d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df = pd.DataFrame(columns=['Model', 'Best Parameters', 'Best Accuracy', 'Time Taken'])\n",
    "\n",
    "def train_and_evaluate_model(model, params, X_train, y_train, X_test, y_test, model_name):\n",
    "    print(f\"Training {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    clf = GridSearchCV(model, params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        clf.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Enregistrement des paramètres et des résultats dans le DataFrame\n",
    "    results_df.loc[len(results_df)] = [model_name, clf.best_params_, clf.best_score_, elapsed_time]\n",
    "    \n",
    "    # Création du sous-répertoire clf_reports et clf_imb_reports s'ils n'existent pas déjà\n",
    "    clf_reports_dir = os.path.join(save_dir, \"clf_reports\")\n",
    "    if not os.path.exists(clf_reports_dir):\n",
    "        os.makedirs(clf_reports_dir)\n",
    "    \n",
    "    # Sauvegarde du rapport de classification sur les données de test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "    report_path = os.path.join(clf_reports_dir, f\"{model_name}_classification_report_{time.strftime('%Y%m%d%H%M%S')}.csv\")\n",
    "    report_df.to_csv(report_path)\n",
    "    print(f\"Classification report saved at: {report_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          02 - 1 Préparation des paramètres pour la grille de recherche GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 100, 'penalty': 'elasticnet', 'solver': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 10, 'penalty': 'elasticnet', 'solver': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NearestCentroid</td>\n",
       "      <td>{'metric': 'euclidean', 'shrink_threshold': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model                                         Parameters\n",
       "0    LogisticRegression  {'C': 100, 'penalty': 'elasticnet', 'solver': ...\n",
       "1    LogisticRegression     {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
       "2    LogisticRegression  {'C': 10, 'penalty': 'elasticnet', 'solver': '...\n",
       "3    LogisticRegression      {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
       "4       NearestCentroid  {'metric': 'euclidean', 'shrink_threshold': None}\n",
       "..                  ...                                                ...\n",
       "165      LGBMClassifier  {'boosting_type': 'dart', 'class_weight': 'bal...\n",
       "166      LGBMClassifier  {'boosting_type': 'dart', 'class_weight': 'bal...\n",
       "167      LGBMClassifier  {'boosting_type': 'dart', 'class_weight': 'bal...\n",
       "168      LGBMClassifier  {'boosting_type': 'dart', 'class_weight': 'bal...\n",
       "169      LGBMClassifier  {'boosting_type': 'dart', 'class_weight': 'bal...\n",
       "\n",
       "[170 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Affichage des modèles entraînes (147 combinaisons de modèles environ)\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Définition de tous les paramètres possibles pour chaque modèle\n",
    "lr_params = {'C': [100, 10], 'solver': ['lbfgs'], 'penalty': ['elasticnet', 'l2']}\n",
    "nc_params = {'metric': ['euclidean'], 'shrink_threshold': [None, 0.5]}\n",
    "etc_params = {'criterion': ['gini', 'entropy'], 'n_estimators': [100, 200], 'max_depth': [None, 30], 'min_samples_split': [2], 'max_features': ['auto', 'sqrt', 'log2'], 'bootstrap': [True], 'oob_score': [True, False]}\n",
    "rfc_params = {'criterion': ['gini', 'entropy'], 'n_estimators': [200, 100, 500], 'max_depth': [None, 30], 'min_samples_split': [5], 'max_features': ['auto', 'sqrt', 'log2'], 'bootstrap': [True], 'oob_score': [True, False]}\n",
    "xgb_params = {'n_estimators': [200, 100, 300], 'max_depth': [5], 'learning_rate': [0.1], 'gamma': [0.2],  'reg_alpha': [0, 0.5],'reg_lambda': [0, 0.5]}\n",
    "lgbm_params = {'boosting_type': ['gbdt', 'dart'], 'n_estimators': [200, 100], 'max_depth': [10, 5], 'learning_rate': [0.1],\n",
    "                'min_child_samples': [10, 50], 'class_weight': [None, 'balanced'], 'num_leaves': [31], 'force_row_wise': [True], 'force_col_wise': [False]}\n",
    "\n",
    "# Création d'un DataFrame pour afficher toutes les combinaisons de paramètres\n",
    "params_combinations = []\n",
    "\n",
    "for model, params in zip(['LogisticRegression', 'NearestCentroid', 'ExtraTreesClassifier', 'RandomForestClassifier', 'XGBClassifier', 'LGBMClassifier'], \n",
    "                         [lr_params, nc_params, etc_params, rfc_params, xgb_params, lgbm_params]):\n",
    "    param_grid = list(ParameterGrid(params))\n",
    "    params_combinations.extend([(model, param) for param in param_grid])\n",
    "\n",
    "params_df = pd.DataFrame(params_combinations, columns=['Model', 'Parameters'])\n",
    "params_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "Classification report saved at: models\\clf_reports\\LogisticRegression_classification_report_20240515220228.csv\n",
      "Training NearestCentroid...\n",
      "Classification report saved at: models\\clf_reports\\NearestCentroid_classification_report_20240515220229.csv\n",
      "Training ExtraTreesClassifier...\n",
      "Classification report saved at: models\\clf_reports\\ExtraTreesClassifier_classification_report_20240515221254.csv\n",
      "Training RandomForestClassifier...\n",
      "Classification report saved at: models\\clf_reports\\RandomForestClassifier_classification_report_20240515223644.csv\n",
      "Training XGBClassifier...\n",
      "Classification report saved at: models\\clf_reports\\XGBClassifier_classification_report_20240515230013.csv\n",
      "Training LGBMClassifier...\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 77595\n",
      "[LightGBM] [Info] Number of data points in the train set: 5687, number of used features: 4797\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Classification report saved at: models\\clf_reports\\LGBMClassifier_classification_report_20240515231031.csv\n",
      "Results saved at: models\\best_models_results_3.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Instantiation des modèles : \n",
    "lr = LogisticRegression()\n",
    "nc = NearestCentroid()\n",
    "etc = ExtraTreesClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# Entraîner et évaluer chaque modèle\n",
    "train_and_evaluate_model(lr, lr_params, X_train_tfidf, y_train_encoded, X_test_tfidf, y_test_encoded, \"LogisticRegression\")\n",
    "train_and_evaluate_model(nc, nc_params, X_train_tfidf, y_train_encoded, X_test_tfidf, y_test_encoded, \"NearestCentroid\")\n",
    "train_and_evaluate_model(etc, etc_params, X_train_tfidf, y_train_encoded, X_test_tfidf, y_test_encoded, \"ExtraTreesClassifier\")\n",
    "train_and_evaluate_model(rfc, rfc_params, X_train_tfidf, y_train_encoded, X_test_tfidf, y_test_encoded, \"RandomForestClassifier\")\n",
    "train_and_evaluate_model(xgb, xgb_params, X_train_tfidf, y_train_encoded, X_test_tfidf, y_test_encoded, \"XGBClassifier\")\n",
    "train_and_evaluate_model(lgbm, lgbm_params, X_train_tfidf, y_train_encoded, X_test_tfidf, y_test_encoded, \"LGBMClassifier\")\n",
    "\n",
    "\n",
    "suffix_num = 1\n",
    "while os.path.exists(os.path.join(save_dir, f\"best_models_results_{suffix_num}.csv\")):\n",
    "    suffix_num += 1\n",
    "\n",
    "# Enregistrer les résultats dans un fichier CSV avec le suffixe numérique\n",
    "results_filename = f\"best_models_results_{suffix_num}.csv\"\n",
    "results_path = os.path.join(save_dir, results_filename)\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"Results saved at: {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin vers le répertoire contenant les fichiers de classification report\n",
    "\n",
    "clf_reports_dir = os.path.join(save_dir, \"clf_reports\")\n",
    "if not os.path.exists(clf_reports_dir):\n",
    "    os.makedirs(clf_reports_dir)\n",
    "\n",
    "# Initialisation d'une liste pour stocker les DataFrames de classification reports\n",
    "dfs = []\n",
    "\n",
    "# Parcourir tous les fichiers CSV dans le répertoire\n",
    "for filename in os.listdir(clf_reports_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Lire le fichier CSV dans un DataFrame\n",
    "        df = pd.read_csv(os.path.join(clf_reports_dir, filename))\n",
    "        \n",
    "        # Extraire le nom du modèle à partir du nom du fichier\n",
    "        model_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Ajouter le nom du modèle comme nouvelle colonne dans le DataFrame\n",
    "        df['Model'] = model_name\n",
    "        \n",
    "        # Ajouter le DataFrame à la liste\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concaténer tous les DataFrames de classification reports en un seul DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# Afficher le DataFrame combiné\n",
    "results_path = os.path.join(save_dir, \"classification_reports_combined.csv\")\n",
    "\n",
    "combined_df.to_csv(results_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.904943</td>\n",
       "      <td>0.946322</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.584071</td>\n",
       "      <td>0.663317</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.941504</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.632242</td>\n",
       "      <td>0.972868</td>\n",
       "      <td>0.766412</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.961373</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.919918</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.845992</td>\n",
       "      <td>0.845992</td>\n",
       "      <td>0.845992</td>\n",
       "      <td>0.845992</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.874435</td>\n",
       "      <td>0.782478</td>\n",
       "      <td>0.804972</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.870893</td>\n",
       "      <td>0.845992</td>\n",
       "      <td>0.846364</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>ExtraTreesClassifier_classification_report_202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.971660</td>\n",
       "      <td>0.912548</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>0.561947</td>\n",
       "      <td>0.631841</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.962428</td>\n",
       "      <td>0.883289</td>\n",
       "      <td>0.921162</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0.633952</td>\n",
       "      <td>0.926357</td>\n",
       "      <td>0.752756</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.923711</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.813592</td>\n",
       "      <td>0.800399</td>\n",
       "      <td>0.799977</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.857034</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>0.839749</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>LGBMClassifier_classification_report_202405152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.980080</td>\n",
       "      <td>0.935361</td>\n",
       "      <td>0.957198</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.681416</td>\n",
       "      <td>0.740385</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.912467</td>\n",
       "      <td>0.928475</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>0.682081</td>\n",
       "      <td>0.914729</td>\n",
       "      <td>0.781457</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>0.962343</td>\n",
       "      <td>0.905512</td>\n",
       "      <td>0.933063</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.865431</td>\n",
       "      <td>0.823399</td>\n",
       "      <td>0.837465</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>0.870481</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>LogisticRegression_classification_report_20240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>0.844106</td>\n",
       "      <td>0.907975</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.425447</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.587106</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>0.970199</td>\n",
       "      <td>0.777188</td>\n",
       "      <td>0.863034</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>0.879032</td>\n",
       "      <td>0.422481</td>\n",
       "      <td>0.570681</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>0.950673</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.755977</td>\n",
       "      <td>0.755977</td>\n",
       "      <td>0.755977</td>\n",
       "      <td>0.755977</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.795972</td>\n",
       "      <td>0.732251</td>\n",
       "      <td>0.730978</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.755977</td>\n",
       "      <td>0.769944</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>NearestCentroid_classification_report_20240515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.912548</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.659898</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>0.941341</td>\n",
       "      <td>0.893899</td>\n",
       "      <td>0.917007</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>0.635443</td>\n",
       "      <td>0.972868</td>\n",
       "      <td>0.768760</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5</td>\n",
       "      <td>0.965665</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.924025</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.876366</td>\n",
       "      <td>0.793848</td>\n",
       "      <td>0.815484</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.871902</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.848558</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>RandomForestClassifier_classification_report_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0.979508</td>\n",
       "      <td>0.908745</td>\n",
       "      <td>0.942801</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.539823</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>0.883289</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>0.605063</td>\n",
       "      <td>0.926357</td>\n",
       "      <td>0.732006</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.829817</td>\n",
       "      <td>0.829817</td>\n",
       "      <td>0.829817</td>\n",
       "      <td>0.829817</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.827348</td>\n",
       "      <td>0.777139</td>\n",
       "      <td>0.790584</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.852725</td>\n",
       "      <td>0.829817</td>\n",
       "      <td>0.831632</td>\n",
       "      <td>1422.000000</td>\n",
       "      <td>XGBClassifier_classification_report_2024051523...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  precision    recall  f1-score      support  \\\n",
       "0              0   0.991667  0.904943  0.946322   263.000000   \n",
       "1              1   0.767442  0.584071  0.663317   226.000000   \n",
       "2              2   0.941504  0.896552  0.918478   377.000000   \n",
       "3              3   0.952381  0.454545  0.615385    44.000000   \n",
       "4              4   0.632242  0.972868  0.766412   258.000000   \n",
       "5              5   0.961373  0.881890  0.919918   254.000000   \n",
       "6       accuracy   0.845992  0.845992  0.845992     0.845992   \n",
       "7      macro avg   0.874435  0.782478  0.804972  1422.000000   \n",
       "8   weighted avg   0.870893  0.845992  0.846364  1422.000000   \n",
       "9              0   0.971660  0.912548  0.941176   263.000000   \n",
       "10             1   0.721591  0.561947  0.631841   226.000000   \n",
       "11             2   0.962428  0.883289  0.921162   377.000000   \n",
       "12             3   0.622222  0.636364  0.629213    44.000000   \n",
       "13             4   0.633952  0.926357  0.752756   258.000000   \n",
       "14             5   0.969697  0.881890  0.923711   254.000000   \n",
       "15      accuracy   0.837553  0.837553  0.837553     0.837553   \n",
       "16     macro avg   0.813592  0.800399  0.799977  1422.000000   \n",
       "17  weighted avg   0.857034  0.837553  0.839749  1422.000000   \n",
       "18             0   0.980080  0.935361  0.957198   263.000000   \n",
       "19             1   0.810526  0.681416  0.740385   226.000000   \n",
       "20             2   0.945055  0.912467  0.928475   377.000000   \n",
       "21             3   0.812500  0.590909  0.684211    44.000000   \n",
       "22             4   0.682081  0.914729  0.781457   258.000000   \n",
       "23             5   0.962343  0.905512  0.933063   254.000000   \n",
       "24      accuracy   0.869198  0.869198  0.869198     0.869198   \n",
       "25     macro avg   0.865431  0.823399  0.837465  1422.000000   \n",
       "26  weighted avg   0.881426  0.869198  0.870481  1422.000000   \n",
       "27             0   0.982301  0.844106  0.907975   263.000000   \n",
       "28             1   0.425447  0.946903  0.587106   226.000000   \n",
       "29             2   0.970199  0.777188  0.863034   377.000000   \n",
       "30             3   0.568182  0.568182  0.568182    44.000000   \n",
       "31             4   0.879032  0.422481  0.570681   258.000000   \n",
       "32             5   0.950673  0.834646  0.888889   254.000000   \n",
       "33      accuracy   0.755977  0.755977  0.755977     0.755977   \n",
       "34     macro avg   0.795972  0.732251  0.730978  1422.000000   \n",
       "35  weighted avg   0.853391  0.755977  0.769944  1422.000000   \n",
       "36             0   0.983607  0.912548  0.946746   263.000000   \n",
       "37             1   0.773810  0.575221  0.659898   226.000000   \n",
       "38             2   0.941341  0.893899  0.917007   377.000000   \n",
       "39             3   0.958333  0.522727  0.676471    44.000000   \n",
       "40             4   0.635443  0.972868  0.768760   258.000000   \n",
       "41             5   0.965665  0.885827  0.924025   254.000000   \n",
       "42      accuracy   0.848101  0.848101  0.848101     0.848101   \n",
       "43     macro avg   0.876366  0.793848  0.815484  1422.000000   \n",
       "44  weighted avg   0.871902  0.848101  0.848558  1422.000000   \n",
       "45             0   0.979508  0.908745  0.942801   263.000000   \n",
       "46             1   0.717647  0.539823  0.616162   226.000000   \n",
       "47             2   0.946023  0.883289  0.913580   377.000000   \n",
       "48             3   0.741935  0.522727  0.613333    44.000000   \n",
       "49             4   0.605063  0.926357  0.732006   258.000000   \n",
       "50             5   0.973913  0.881890  0.925620   254.000000   \n",
       "51      accuracy   0.829817  0.829817  0.829817     0.829817   \n",
       "52     macro avg   0.827348  0.777139  0.790584  1422.000000   \n",
       "53  weighted avg   0.852725  0.829817  0.831632  1422.000000   \n",
       "\n",
       "                                                Model  \n",
       "0   ExtraTreesClassifier_classification_report_202...  \n",
       "1   ExtraTreesClassifier_classification_report_202...  \n",
       "2   ExtraTreesClassifier_classification_report_202...  \n",
       "3   ExtraTreesClassifier_classification_report_202...  \n",
       "4   ExtraTreesClassifier_classification_report_202...  \n",
       "5   ExtraTreesClassifier_classification_report_202...  \n",
       "6   ExtraTreesClassifier_classification_report_202...  \n",
       "7   ExtraTreesClassifier_classification_report_202...  \n",
       "8   ExtraTreesClassifier_classification_report_202...  \n",
       "9   LGBMClassifier_classification_report_202405152...  \n",
       "10  LGBMClassifier_classification_report_202405152...  \n",
       "11  LGBMClassifier_classification_report_202405152...  \n",
       "12  LGBMClassifier_classification_report_202405152...  \n",
       "13  LGBMClassifier_classification_report_202405152...  \n",
       "14  LGBMClassifier_classification_report_202405152...  \n",
       "15  LGBMClassifier_classification_report_202405152...  \n",
       "16  LGBMClassifier_classification_report_202405152...  \n",
       "17  LGBMClassifier_classification_report_202405152...  \n",
       "18  LogisticRegression_classification_report_20240...  \n",
       "19  LogisticRegression_classification_report_20240...  \n",
       "20  LogisticRegression_classification_report_20240...  \n",
       "21  LogisticRegression_classification_report_20240...  \n",
       "22  LogisticRegression_classification_report_20240...  \n",
       "23  LogisticRegression_classification_report_20240...  \n",
       "24  LogisticRegression_classification_report_20240...  \n",
       "25  LogisticRegression_classification_report_20240...  \n",
       "26  LogisticRegression_classification_report_20240...  \n",
       "27  NearestCentroid_classification_report_20240515...  \n",
       "28  NearestCentroid_classification_report_20240515...  \n",
       "29  NearestCentroid_classification_report_20240515...  \n",
       "30  NearestCentroid_classification_report_20240515...  \n",
       "31  NearestCentroid_classification_report_20240515...  \n",
       "32  NearestCentroid_classification_report_20240515...  \n",
       "33  NearestCentroid_classification_report_20240515...  \n",
       "34  NearestCentroid_classification_report_20240515...  \n",
       "35  NearestCentroid_classification_report_20240515...  \n",
       "36  RandomForestClassifier_classification_report_2...  \n",
       "37  RandomForestClassifier_classification_report_2...  \n",
       "38  RandomForestClassifier_classification_report_2...  \n",
       "39  RandomForestClassifier_classification_report_2...  \n",
       "40  RandomForestClassifier_classification_report_2...  \n",
       "41  RandomForestClassifier_classification_report_2...  \n",
       "42  RandomForestClassifier_classification_report_2...  \n",
       "43  RandomForestClassifier_classification_report_2...  \n",
       "44  RandomForestClassifier_classification_report_2...  \n",
       "45  XGBClassifier_classification_report_2024051523...  \n",
       "46  XGBClassifier_classification_report_2024051523...  \n",
       "47  XGBClassifier_classification_report_2024051523...  \n",
       "48  XGBClassifier_classification_report_2024051523...  \n",
       "49  XGBClassifier_classification_report_2024051523...  \n",
       "50  XGBClassifier_classification_report_2024051523...  \n",
       "51  XGBClassifier_classification_report_2024051523...  \n",
       "52  XGBClassifier_classification_report_2024051523...  \n",
       "53  XGBClassifier_classification_report_2024051523...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
